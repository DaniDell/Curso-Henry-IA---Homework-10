"""
Chatbot RAG para Casa Mueble
Este script implementa un chatbot basado en recuperaci贸n aumentada de generaci贸n (RAG)
que utiliza una base de conocimientos vectorizada para responder preguntas sobre productos
y servicios de Casa Mueble.
"""
import os
import sys
import logging
import re
from typing import List, Dict, Any
from dotenv import load_dotenv

from langchain_community.vectorstores.faiss import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI  # For fallback to OpenAI
from langchain_community.llms import HuggingFaceHub  # For local LLM

from config import KNOWLEDGE_DIR, INDEX_DIR, EMBEDDING_MODEL_NAME


# Cargar variables de entorno desde .env si existe
load_dotenv(override=True)

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Prompt template para el chatbot
SYSTEM_TEMPLATE = """
Eres un asistente virtual de Casa Mueble, una empresa especializada en muebles de alta calidad. Tu objetivo es ayudar a los clientes a encontrar productos, resolver dudas sobre la empresa y brindar una experiencia personalizada.

INSTRUCCIONES CRTICAS SOBRE ALUCINACIONES:
- NUNCA, BAJO NINGUNA CIRCUNSTANCIA, debes inventar o fabricar informaci贸n que no est茅 expl铆citamente en el contexto proporcionado.
- SOLO menciona productos que existan expl铆citamente en el contexto. La lista completa de productos es: Camastro Leonor, Camastro Clara, Camastro Delfina, Sill贸n Clemente, Kit Barral Simple Completo, Kit Barral Doble Completo, Fogonero Perikles, Fogonero Efesto, Fogonero con Media Parrilla, Media Parrilla, Estaca Asador, Mesa Brisa 100x50 cm, Juego de Mesas Nido Redondas.
- NUNCA menciones productos gen茅ricos como "Mesa de Comedor Extensible" si no est谩n expl铆citamente en el contexto.
- NUNCA inventes precios. Si un precio no est谩 expl铆citamente en el contexto, simplemente di que no tienes esa informaci贸n.
- Si encuentras una pregunta que requiere informaci贸n no disponible en el contexto, simplemente admite que no tienes esa informaci贸n espec铆fica.

INSTRUCCIONES GENERALES:
1. S茅 amable, profesional y conciso en tus respuestas.
2. MUY IMPORTANTE: SIEMPRE que el contexto incluya informaci贸n de productos, muestra primero las opciones concretas (nombre, categor铆a, caracter铆sticas, precio, etc.) relevantes a la consulta del cliente, ANTES de hacer preguntas o pedir aclaraciones.
3. Si hay varios productos relevantes, muestra una lista breve y clara con nombre, categor铆a y precio (si est谩 disponible), y luego ofrece ampliar detalles si el cliente lo desea.
4. CRUCIAL: ANTES de indicar que no tienes informaci贸n sobre un producto espec铆fico, BUSCA CUIDADOSAMENTE en todo el contexto proporcionado, prestando especial atenci贸n a nombres similares, variaciones ortogr谩ficas o productos de la misma categor铆a.
5. Para datos espec铆ficos como precios, dimensiones o disponibilidad, cita la fuente de donde obtienes la informaci贸n.
6. Mant茅n una conversaci贸n fluida, reconociendo y recordando lo que el cliente ya mencion贸 previamente, pero NO retrases la presentaci贸n de productos con preguntas innecesarias.
7. Evita repetir informaci贸n que ya has proporcionado anteriormente.
8. Si te piden un precio espec铆fico y no est谩 en el contexto, NUNCA inventes un precio. En su lugar, di: "Lo siento, no tengo informaci贸n sobre el precio de este producto en particular. Para obtener el precio actualizado, te recomiendo contactar directamente con Casa Mueble a trav茅s de su p谩gina web o n煤mero de atenci贸n al cliente."
9. Si tras una b煤squeda cuidadosa no encuentras informaci贸n sobre un producto espec铆fico, NO INVENTES que el producto existe o tiene ciertas caracter铆sticas. Di claramente que no tienes informaci贸n sobre ese producto.

CONTEXTO RELEVANTE (Esta es tu 煤nica fuente de informaci贸n para responder):
{context}

HISTORIAL DE CONVERSACIN:
{chat_history}

PREGUNTA ACTUAL DEL CLIENTE:
{question}

Tu respuesta debe ser 煤til, relevante y amigable, mostrando primero las opciones concretas de productos o informaci贸n relevante, y solo despu茅s, si es necesario, hacer preguntas para personalizar la atenci贸n.
"""

def load_embeddings() -> HuggingFaceEmbeddings:
    """
    Carga el modelo de embeddings.
    
    Returns:
        HuggingFaceEmbeddings: Modelo de embeddings cargado
    """
    logger.info(f"Cargando modelo de embeddings: {EMBEDDING_MODEL_NAME}")
    
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            cache_folder=os.path.join(os.path.dirname(INDEX_DIR), "models_cache")
        )
        
        return embeddings
    
    except Exception as e:
        logger.error(f"Error al cargar modelo de embeddings: {str(e)}")
        sys.exit(1)

def load_vector_db(embeddings: HuggingFaceEmbeddings) -> FAISS:
    """
    Carga la base de datos vectorial FAISS.
    
    Args:
        embeddings (HuggingFaceEmbeddings): Modelo de embeddings
        
    Returns:
        FAISS: Base de datos vectorial cargada
    """
    logger.info(f"Cargando 铆ndice FAISS desde: {INDEX_DIR}")
    
    try:
        if not os.path.exists(os.path.join(INDEX_DIR, "index.faiss")):
            logger.error(f"No se encontr贸 el 铆ndice FAISS en: {INDEX_DIR}")
            logger.info("Ejecuta 'python manage_knowledge_base.py rebuild' para crear el 铆ndice")
            sys.exit(1)
            
        vector_db = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        logger.info("ndice FAISS cargado exitosamente")
        
        return vector_db
    
    except Exception as e:
        logger.error(f"Error al cargar 铆ndice FAISS: {str(e)}")
        sys.exit(1)

def load_llm():
    """
    Carga el modelo de lenguaje a utilizar.
    
    Returns:
        El modelo de lenguaje cargado o None si ocurre un error
    """
    logger.info("Cargando modelo de lenguaje")    # Primera opci贸n: probar con OpenAI si hay API key disponible
    if os.environ.get("OPENAI_API_KEY"):
        try:
            logger.info("Usando OpenAI como modelo de lenguaje")
            return ChatOpenAI(temperature=0.1, model_name="gpt-3.5-turbo")
        except Exception as e:
            logger.warning(f"Error al cargar OpenAI: {str(e)}")
    
    # Segunda opci贸n: probar con HuggingFace Hub
    if os.environ.get("HUGGINGFACEHUB_API_TOKEN"):
        try:
            logger.info("Usando HuggingFace Hub como modelo de lenguaje")
            return HuggingFaceHub(
                repo_id="google/flan-t5-base",
                model_kwargs={"temperature": 0.1, "max_length": 512}
            )
        except Exception as e:
            logger.warning(f"Error al cargar HuggingFace Hub: {str(e)}")
    
    # Si llegamos aqu铆, advertir que no hay modelo disponible
    logger.warning("No se pudo cargar ning煤n modelo de lenguaje. El sistema funcionar谩 con formato de plantilla simple.")
    return None

def expand_query(query: str, chat_history: List[Dict[str, str]] = None) -> str:
    """
    Expande la consulta del usuario para mejorar la b煤squeda vectorial.
    
    Args:
        query (str): Consulta original del usuario
        chat_history (List[Dict[str, str]], optional): Historial de la conversaci贸n
        
    Returns:
        str: Consulta expandida y mejorada
    """
    expanded_query = query
    
    # A帽adir contexto del historial de chat si est谩 disponible
    if chat_history and len(chat_history) > 0:
        # Tomar las 煤ltimas 2 interacciones para contexto
        recent_context = chat_history[-4:] if len(chat_history) > 4 else chat_history
        context = " ".join([entry["content"] for entry in recent_context])
        
        # Construir una consulta expandida con el contexto
        expanded_query = f"{query}. Contexto adicional de la conversaci贸n: {context}"
    
    # Expandir nombres de productos espec铆ficos
    product_specific_mappings = {
        "camastro": ["camastro", "tumbona", "reposera", "sill贸n reclinable", "leonor", "clara", "delfina"],
        "sill贸n": ["sill贸n", "sillon", "sof谩", "sofa", "butaca", "clemente"],
        "fogonero": ["fogonero", "brasero", "parrilla", "asador", "perikles", "efesto"],
        "mesa": ["mesa", "escritorio", "mueble", "mesita", "brisa"],
        "kit": ["kit", "conjunto", "set", "barral"]
    }
    
    # Comprobar si la consulta contiene palabras clave de productos espec铆ficos
    for product_type, synonyms in product_specific_mappings.items():
        if any(term.lower() in query.lower() for term in synonyms):
            # Si se encuentra un tipo de producto, a帽adir todos sus sin贸nimos a la consulta expandida
            additional_terms = " ".join([term for term in synonyms if term.lower() not in expanded_query.lower()])
            expanded_query = f"{expanded_query} {additional_terms}"
            
            # Si la consulta menciona un nombre espec铆fico, reforzarlo
            for potential_name in ["leonor", "clara", "delfina", "clemente", "perikles", "efesto", "brisa"]:
                if potential_name.lower() in query.lower():
                    expanded_query = f"{expanded_query} producto {potential_name} espec铆fico"
                    break
    
    # A帽adir t茅rminos espec铆ficos para mejorar la b煤squeda
    product_keywords = [
        "mesa", "silla", "mueble", "fogonero", "estante", "rack", 
        "biblioteca", "perchero", "espejo", "camastro", "sill贸n"
    ]
    
    for keyword in product_keywords:
        if keyword in query.lower() and keyword not in expanded_query.lower():
            expanded_query = f"{expanded_query} {keyword}"
    
    # Si la consulta menciona dimensiones o colores, enfatizarlos
    dimension_pattern = r'\d+\s*(?:cm|mts?|metros?|centimetros?)'
    dimensions = re.findall(dimension_pattern, query.lower())
    colors = ["negro", "blanco", "verde", "oxido", "oxidado"]
    
    dimension_terms = " ".join(dimensions)
    color_terms = " ".join([color for color in colors if color in query.lower()])
    
    if dimension_terms:
        expanded_query = f"{expanded_query} con dimensiones {dimension_terms}"
    if color_terms:
        expanded_query = f"{expanded_query} de color {color_terms}"
    
    return expanded_query

def search_knowledge_base(query: str, vector_db: FAISS, k: int = 3, chat_history: List[Dict[str, str]] = None) -> List[str]:
    """
    Busca en la base de conocimientos utilizando la consulta del usuario.
    
    Args:
        query (str): Consulta del usuario
        vector_db (FAISS): Base de datos vectorial
        k (int, optional): N煤mero de documentos a recuperar. Default es 3.
        chat_history (List[Dict[str, str]], optional): Historial de la conversaci贸n
        
    Returns:
        List[str]: Documentos relevantes encontrados
    """
    try:
        # Detectar si la consulta es sobre un producto espec铆fico
        product_specific = False
        product_keywords = ["camastro", "sill贸n", "fogonero", "mesa", "parrilla", "kit", "barral", "estaca"]
        for keyword in product_keywords:
            if keyword.lower() in query.lower():
                product_specific = True
                break
        
        # Ajustar k si la consulta es sobre un producto espec铆fico
        if product_specific:
            k = 5  # Aumentar el n煤mero de resultados para consultas de productos espec铆ficos
        
        # Expand query to improve search
        expanded_query = expand_query(query, chat_history)
        logger.info(f"Consulta original: '{query}' -> Expandida: '{expanded_query}'")
        
        # Realizar la b煤squeda de similitud con la consulta expandida
        documents = vector_db.similarity_search(expanded_query, k=k)
        
        # B煤squeda adicional con la consulta original para no perder resultados directos
        original_documents = vector_db.similarity_search(query, k=k)
        
        # Para productos espec铆ficos, realizar una b煤squeda adicional con palabras clave exactas
        if product_specific:
            # Extraer posibles nombres de productos de la consulta
            words = query.lower().split()
            for word in words:
                if len(word) > 3 and word not in ["que", "cual", "como", "donde", "quien", "tiene", "para"]:
                    # Buscar documentos que contengan exactamente esa palabra
                    exact_documents = vector_db.similarity_search(word, k=3)
                    original_documents.extend(exact_documents)
        
        # Combinar resultados y eliminar duplicados
        all_docs = []
        doc_contents = set()
        
        for doc_list in [documents, original_documents]:
            for doc in doc_list:
                if doc.page_content not in doc_contents:
                    all_docs.append(doc)
                    doc_contents.add(doc.page_content)
        
        # Formatear resultados con fuentes
        contexts = []
        max_results = k + 2 if product_specific else k  # M谩s resultados para productos espec铆ficos
        for doc in all_docs[:max_results]:  # Limitar a max_results despu茅s de combinar
            # Extraer metadata o nombre de archivo como fuente
            source = doc.metadata.get('source', 'Unknown source') if hasattr(doc, 'metadata') else 'Unknown source'
            
            # Formatear el contenido con la fuente
            formatted_content = f"{doc.page_content}\n[Fuente: {source}]"
            contexts.append(formatted_content)
        
        return contexts
    
    except Exception as e:
        logger.error(f"Error al buscar en la base de conocimientos: {str(e)}")
        return []

def format_chat_history(history: List[Dict[str, str]]) -> str:
    """
    Formatea el historial de chat para incluirlo en el prompt.
    
    Args:
        history (List[Dict[str, str]]): Lista de mensajes con claves 'role' y 'content'
        
    Returns:
        str: Historial de chat formateado
    """
    if not history:
        return "No hay historial previo."
        
    formatted_history = ""
    for message in history:
        role = "Cliente" if message["role"] == "user" else "Asistente"
        formatted_history += f"{role}: {message['content']}\n\n"
        
    return formatted_history.strip()

def process_query(user_input: str, vector_db: FAISS, llm=None, chat_history: List[Dict[str, str]] = None) -> str:
    """
    Procesa la consulta del usuario y genera una respuesta utilizando solo la base vectorial y el LLM.
    Si no hay informaci贸n relevante, genera un fallback contextualizado con sugerencia de web y URL personalizada.
    """
    if chat_history is None:
        chat_history = []

    # 1. Expandir la consulta usando historial y sin贸nimos
    expanded_query = expand_query(user_input, chat_history)

    # 2. Buscar en la base vectorial
    knowledge_base_info = search_knowledge_base(expanded_query, vector_db, k=3, chat_history=chat_history)

    # 3. Si hay resultados, armar contexto y generar respuesta con LLM
    if knowledge_base_info:
        context = "\n\n".join(knowledge_base_info)
        formatted_history = format_chat_history(chat_history)
        prompt = ChatPromptTemplate.from_template(SYSTEM_TEMPLATE)
        output_parser = StrOutputParser()
        chain = prompt | llm | output_parser
        response = chain.invoke({
            "context": context,
            "question": user_input,
            "chat_history": formatted_history
        })
        return response    # 4. Si no hay resultados, fallback contextualizado    formatted_history = format_chat_history(chat_history)
    fallback_context = (
        "No se encontr贸 informaci贸n relevante sobre esta consulta en nuestra base de datos. "
        "IMPORTANTE: Debes indicar claramente al cliente que no dispones de informaci贸n espec铆fica sobre su consulta. "
        "NO inventes productos, caracter铆sticas, precios, o cualquier otra informaci贸n. "
        "La lista completa de productos en nuestra base de conocimientos es: Camastro Leonor, Camastro Clara, Camastro Delfina, Sill贸n Clemente, "
        "Kit Barral Simple Completo, Kit Barral Doble Completo, Fogonero Perikles, Fogonero Efesto, Fogonero con Media Parrilla, Media Parrilla, "
        "Estaca Asador, Mesa Brisa 100x50 cm, Juego de Mesas Nido Redondas. "
        "Recomienda al cliente visitar la web oficial: https://casamueble.com.ar"
    )
    # Si la consulta es sobre producto, armar URL personalizada
    product_keywords = ["producto", "mueble", "mesa", "silla", "fogonero", "camastro"]
    if any(word in user_input.lower() for word in product_keywords):
        fallback_context += f"\nTambi茅n pod茅s sugerir que busque aqu铆: https://casamueble.com.ar/search/?q={user_input.replace(' ', '+')}"

    prompt = ChatPromptTemplate.from_template(SYSTEM_TEMPLATE)
    output_parser = StrOutputParser()
    
    # Si estamos usando LLM, utilizar la cadena normal
    if llm:
        chain = prompt | llm | output_parser
        response = chain.invoke({
            "context": fallback_context,
            "question": user_input,
            "chat_history": formatted_history
        })
    else:
        # Si no hay LLM disponible, usar una respuesta predeterminada para evitar hallucinations
        response = (
            "Lo siento, no tengo informaci贸n espec铆fica sobre tu consulta en mi base de datos. "
            "Para obtener informaci贸n actualizada y precisa, te recomiendo visitar la p谩gina web oficial "
            "de Casa Mueble en https://casamueble.com.ar o contactar directamente con el servicio de atenci贸n al cliente."
        )
    return response

def save_conversation_log(chat_history, filename="conversation_log.txt"):
    """
    Guarda el historial de la conversaci贸n en un archivo de texto.
    Args:
        chat_history (list): Lista de mensajes (dicts con 'role' y 'content')
        filename (str): Nombre del archivo donde guardar el log
    """
    # Crear ruta absoluta para el archivo de log
    log_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
    
    try:
        with open(log_path, "a", encoding="utf-8") as f:
            f.write("\n=== Nueva conversaci贸n ===\n")
            f.write(f"Fecha y hora: {os.path.basename(__file__)} - {os.path.dirname(os.path.abspath(__file__))}\n")
            for msg in chat_history:
                role = "Usuario" if msg["role"] == "user" else "Asistente"
                f.write(f"{role}: {msg['content']}\n")
            f.write("\n=== Fin de la conversaci贸n ===\n\n")
            
        # Imprimir la ubicaci贸n del archivo para referencia
        print(f"\n[Log guardado en: {log_path}]")
        
    except Exception as e:
        logger.error(f"Error al guardar el log: {str(e)}")
        print(f"\n[Error al guardar el log: {str(e)}]")

def main():
    """Funci贸n principal para ejecutar el chatbot."""
    print("\n===== Bienvenido al Asistente Virtual de Casa Mueble =====")
    print("Escribe 'salir' o 'exit' para terminar la conversaci贸n.")
    print("驴En qu茅 puedo ayudarte hoy?\n")
    
    # Cargar embeddings, base de datos vectorial y modelo de lenguaje
    embeddings = load_embeddings()
    vector_db = load_vector_db(embeddings)
    
    # Cargar el modelo de lenguaje una sola vez
    try:
        llm = load_llm()
        logger.info("Modelo de lenguaje cargado correctamente")
    except Exception as e:
        logger.error(f"Error al cargar el modelo de lenguaje: {str(e)}")
        llm = None
        
    # Inicializar historial de conversaci贸n
    chat_history = []
    hubo_conversacion = False
    while True:
        # Obtener entrada del usuario
        user_input = input("\n T煤: ")
        
        # Verificar si el usuario quiere salir
        if user_input.lower() in ["salir", "exit", "quit"]:
            print("\n Asistente: 隆Gracias por utilizar nuestro asistente virtual! 隆Hasta pronto!")
            # Guardar log solo si hubo al menos un intercambio exitoso
            if hubo_conversacion and len(chat_history) > 1:
                save_conversation_log(chat_history)
                print("\n[Conversaci贸n registrada en conversation_log.txt]")
            break
        
        # Agregar entrada del usuario al historial
        chat_history.append({"role": "user", "content": user_input})
        
        # Limitar tama帽o del historial (煤ltimo par de intercambios)
        if len(chat_history) > 10:
            chat_history = chat_history[-10:]
        
        # Procesar la consulta y generar respuesta
        try:
            response = process_query(user_input, vector_db, llm, chat_history)
            print(f"\n Asistente: {response}")
            
            # Agregar respuesta al historial
            chat_history.append({"role": "assistant", "content": response})
            hubo_conversacion = True
        except Exception as e:
            logger.error(f"Error al procesar la consulta: {str(e)}")
            error_msg = "Lo siento, ha ocurrido un error al procesar tu consulta. Por favor, intenta de nuevo."
            print(f"\n Asistente: {error_msg}")
            
            # Tambi茅n agregar mensaje de error al historial
            chat_history.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main()
